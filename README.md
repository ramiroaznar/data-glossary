# Data Glossary

* Data glossary curated by [@ramiroaznar](https://twitter.com/ramiroaznar)
* [Original Spreadsheet](https://docs.google.com/spreadsheets/d/1DIh6jLGLvCGK093BBgq52yqIk1M1z9vF3yjqouYdmc4/edit#gid=0)

## References [wip]

Most of the references used in this glossary come from Wikipedia and the first search results in Google.

## List

* **data accuracy**: Data accuracy is one of the components of data quality. It refers to whether the data values stored for an object are the correct values. To be correct, a data values must be the right value and must be represented in a consistent and unambiguous form.

* **data architecture**: Data architecture is a framework for how IT infrastructure supports your data strategy. The goal of any data architecture is to show the company's infrastructure how data is acquired, transported, stored, queried, and secured. A data architecture is the foundation of any data strategy.

* **data analysis**: Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data. A data analyst collects, cleans, and interprets data sets in order to answer a question or solve a problemd **data anomaly**: Data anomalies are rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.
  
* **database**: A database is an organized collection of data stored and accessed electronically from a computer system.

* **big data**: The definition of big data is data that contains greater variety, arriving in increasing volumes and with more velocity. This is also known as the three Vs.

* **data bug**: A bug in the code or query that generates wrong data in any step of the modern data stack.

* **data center**: A data center or data centre is a building, a dedicated space within a building, or a group of buildings used to house computer systems and associated components, such as telecommunications and storage systems.

* **data cleansing**: Data cleansing or data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.

* **data completeness**: Data completeness refers to the comprehensiveness or wholeness of the data. There should be no gaps or missing information for data to be truly complete. Sometimes incomplete data is unusable, but often it's still used even with missing information, which can lead to costly mistakes and false conclusions.

* **data cube**: A data cube refers is a three-dimensional (3D) (or higher) range of values that are generally used to explain the time sequence of an image's data.

* **derived data**: Derived data is data that can be computed from other base data.

* **data downtime**: Data downtime refers to periods of time when your data is partial, erroneous, missing or otherwise inaccurate.

* **data driven**: When a company employs a “data-driven” approach, it means it makes strategic decisions based on data analysis and interpretation. A data-driven approach enables companies to examine and organise their data with the goal of better serving their customers and consumers.

* **data dump**: A database dump contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements. A database dump is most often used for backing up a database so that its contents can be restored in the event of data loss.

* **data ecosystem**: A data ecosystem is a collection of infrastructure, analytics, and applications used to capture and analyze datad **data engineering**: Data engineering is the aspect of data science that focuses on practical applications of data collection and analysis. Data engineers built ETL pipelines and maintain databases.

* **event data**: Event data is any data that you want to measure about an event.

* **exploratory data analysis**: Exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.

* **data format**: A data format or data type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.

* **dataframe**: DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.

* **data governance**: Data governance is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.

* **data infrastructure**: Data Infrastructure can be seen as a complete technology, process, or a whole set up to store, maintain, organize, and distribute it in the form of insightful information.

* **data insights**: Data insights are knowledge that a company gains from analyzing sets of information pertaining to a given topic or situationd **data lake**: A data lake is a storage repository that holds a vast amount of raw data in its native format until it is needed.

* **data leak**: A data leak, spill or breach is the intentional or unintentional release of secure or private/confidential information to an untrusted environment.

* **data legacy**: Historical data that are used by a legacy system that could be defined as a long-term mission-critical system that performs important business functions and contains comprehensive business knowledge.

* **linked data**: Linked data is structured data which is interlinked with other data so it becomes more useful through semantic queries.

* **data management**: Data management is the practice of collecting, keeping, and using data securely, efficiently, and cost-effectively.

* **data mart**: A data mart is a structure / access pattern specific to data warehouse environments, used to retrieve client-facing data.

* **data mesh**: Data system that tries to achieve the promise of scale, while delivering quality and integrity guarantees needed to make data usable : 1) domain-oriented decentralized data ownership and architecture, 2) data as a product, 3) self-serve data infrastructure as a platform, and 4) federated computational governance.

* **metadata**: Metadata is data about data. In other words, it's information that's used to describe the data that's contained in something like a web page, document, or file.

* **data migration**: Data migration or database migration is the process of selecting, preparing, extracting, and transforming data and permanently transferring it from one computer storage system to another.

* **data mining**: Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.

* **data model**: A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities.

* **data monetization**: Data Monetization refers to the process of using data to obtain quantifiable economic benefit.

* **data monitoring**: Data monitoring is a business practice in which critical business data is routinely checked against quality control rules to make sure it is always of high quality and meets previously established standards for formatting and consistency.

* **open data**: Open Data is the idea that some data should be freely available to everyone to use and republish as they wish, without restrictions from copyright, patents or other mechanisms of control.

* **personal data**: Personal data is information that relates to an identified or identifiable individual.

* **data pipeline**: A pipeline or data pipeline is a set of data processing elements connected in series, where the output of one element is the input of the next one.

* **data platform**: A data platform is an integrated technology solution that allows data located in database(s) to be governed, accessed, and delivered to users, data applications, or other technologies for strategic business purposes.

* **data point**: A data point or observation is a set of one or more measurements on a single member of the unit of observation.

* **open data portal**: An open data portal is any online platform which supports users in accessing collections of open data.

* **data product**: A data product is an application or tool that uses data to help businesses improve their decisions and processes.

* **data quality**: Data quality is a measure of the condition of data based on factors such as accuracy, completeness, consistency, reliability and whether it's up to date.

* **real-time data**: Real-time data is information that is delivered immediately after collection. There is no delay in the timeliness of the information provided.

* **data reporting**: Data reporting is the process of collecting and formatting raw data and translating it into a digestible format to assess the ongoing performance of your organization. Your data reports can answer basic questions about the state of your business.

* **data schema**: A database schema is the skeleton structure that represents the logical view of the entire database. It defines how the data is organized and how the relations among them are associated. It formulates all the constraints that are to be applied on the data.

* **data science**: Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.

* **data scraping**: Data scraping is a technique where a computer program extracts data from human-readable output coming from another program.

* **data security**: Data security refers to the process of protecting data from unauthorized access and data corruption throughout its lifecycle. Data security includes data encryption, hashing, tokenization, and key management practices that protect data across all applications and platforms.

* **data set**: A data set (or dataset) is a collection of data.

* **spatial data**: Geospatial, spatial, geographic or location data is information that describes objects, events or other features with a location on or near the surface of the earth. Geospatial data typically combines location information (usually coordinates on the earth) and attribute information (the characteristics of the object, event or phenomena concerned) with temporal information (the time or life span at which the location and attributes exist).

* **data specification**: A data definition specification is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition.

* **data specialization**: The different roles and positions (data scientist, data engineer, analytics engineer, ML engineer...) that are related to the modern data stack. It is also associated with the degree of specialization from generalists (almost full-stack engineers) to especialists.

* **modern data stack**: The modern data stack (MDS) is a suite of tools used for data integration. These tools include, in order of how the data flows: a fully managed ELT data pipeline, a cloud-based columnar warehouse or data lake as a destination, a data transformation tool, and a business intelligence or data visualization platform.

* **data storage**: Data storage refers to the use of recording media to retain data using computers or other devices.

* **data structure**: A data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.

* **structured data**: Structured data is data that adheres to a pre-defined data model and is therefore straightforward to analyse. Structured data conforms to a tabular format with relationship between the different rows and columns. Common examples of structured data are Excel files or SQL databases.

* **data synchronization**: Data synchronization is the process of establishing consistency among data from a source to a target data storage and vice versa and the continuous harmonization of the data over time.

* **data team**: The group of people within a certain organization that is in charge of maintaining the modern data stack.

* **test data**: Test data is data which has been specifically identified for use in tests, typically of a computer program. Some data may be used in a confirmatory way, typically to verify that a given set of input to a given function produces some expected result.

* **data type**: A data type is a classification of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support various types of data, including integer, real, character or string, and Boolean.

* **unstructured data**: Unstructured data is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well.

* **data visualization**: Data visualization is an interdisciplinary field that deals with the graphic representation of data. It is a particularly efficient way of communicating when the data is numerous as for example a time series.

* **data warehouse**: A data warehouse is a type of data management system that is designed to enable and support business intelligence (BI) activities, especially analytics. Data warehouses are solely intended to perform queries and analysis and often contain large amounts of historical data.
